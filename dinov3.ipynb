{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571fdb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 1677 1514\n",
      "Patch size: 16\n",
      "Num register tokens: 4\n",
      "Preprocessed image size: torch.Size([1, 3, 224, 224])\n",
      "{'pixel_values': tensor([[[[-0.4739, -0.0458, -0.0458,  ...,  0.9132,  0.7077,  0.9817],\n",
      "          [-1.0904, -1.2445, -1.2788,  ...,  0.8789,  0.7933,  0.8447],\n",
      "          [-1.5014, -1.4158, -1.4843,  ...,  0.8618,  0.6563,  0.5878],\n",
      "          ...,\n",
      "          [-0.8678, -0.8678, -0.9020,  ..., -1.4672, -1.4500, -1.4329],\n",
      "          [-0.8849, -0.8678, -0.8507,  ..., -1.4500, -1.4500, -1.4500],\n",
      "          [-0.8507, -0.8335, -0.8335,  ..., -1.4500, -1.4500, -1.4672]],\n",
      "\n",
      "         [[ 0.1176,  0.5553,  0.5728,  ...,  0.8704,  0.7479,  1.0805],\n",
      "          [-0.4601, -0.6176, -0.7227,  ...,  0.8529,  0.8004,  0.8704],\n",
      "          [-1.0553, -1.0028, -1.0378,  ...,  0.7654,  0.6604,  0.6078],\n",
      "          ...,\n",
      "          [-0.4601, -0.4601, -0.4951,  ..., -1.1604, -1.1429, -1.1429],\n",
      "          [-0.4776, -0.4601, -0.4426,  ..., -1.1429, -1.1429, -1.1604],\n",
      "          [-0.4426, -0.4251, -0.4251,  ..., -1.1429, -1.1604, -1.1604]],\n",
      "\n",
      "         [[ 0.2696,  0.7228,  0.7054,  ...,  0.8448,  0.6531,  0.9319],\n",
      "          [-0.6715, -0.7936, -0.6715,  ...,  0.8797,  0.7576,  0.7925],\n",
      "          [-0.9853, -0.8110, -0.7936,  ...,  0.7402,  0.6008,  0.5485],\n",
      "          ...,\n",
      "          [-0.1661, -0.1661, -0.1835,  ..., -0.7936, -0.7587, -0.7413],\n",
      "          [-0.1835, -0.1661, -0.1487,  ..., -0.7761, -0.7761, -0.7587],\n",
      "          [-0.1487, -0.1312, -0.1312,  ..., -0.7587, -0.7587, -0.7587]]]])}\n",
      "torch.Size([1, 201, 1024])\n",
      "cls shape: torch.Size([1, 1024])\n",
      "patch features shape: torch.Size([1, 14, 14, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from transformers.image_utils import load_image\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "from keys import token\n",
    "login(token=token)\n",
    "\n",
    "image_path = r\"D:\\KIMKEK-EXTRA\\dataset_validation\\scenes\\03_Cam2_MorningPeak\\images\\frame360.png\"\n",
    "assert os.path.isfile(image_path), f\"Image not found: {image_path}\"\n",
    "image = load_image(image_path)\n",
    "print(\"Image size:\", image.height, image.width)\n",
    "\n",
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "# pipe = pipeline(\"image-feature-extraction\", model=\"facebook/dinov3-vitl16-pretrain-lvd1689m\")\n",
    "\n",
    "from transformers import ViTImageProcessor, AutoModel\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\"facebook/dinov3-vitl16-pretrain-lvd1689m\")\n",
    "model = AutoModel.from_pretrained(\"facebook/dinov3-vitl16-pretrain-lvd1689m\")\n",
    "\n",
    "\n",
    "patch_size = model.config.patch_size\n",
    "print(\"Patch size:\", patch_size) # 16\n",
    "print(\"Num register tokens:\", model.config.num_register_tokens) # 4\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "print(\"Preprocessed image size:\", inputs.pixel_values.shape)  # [1, 3, 224, 224]\n",
    "\n",
    "batch_size, _, img_height, img_width = inputs.pixel_values.shape\n",
    "num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n",
    "num_patches_flat = num_patches_height * num_patches_width\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "with torch.inference_mode():\n",
    "  outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.shape)  # [1, 1 + 4 + 256, 384]\n",
    "assert last_hidden_states.shape == (batch_size, 1 + model.config.num_register_tokens + num_patches_flat, model.config.hidden_size)\n",
    "\n",
    "cls_token = last_hidden_states[:, 0, :]\n",
    "patch_features_flat = last_hidden_states[:, 1 + model.config.num_register_tokens:, :]\n",
    "patch_features = patch_features_flat.unflatten(1, (num_patches_height, num_patches_width))\n",
    "\n",
    "print(\"cls shape:\", cls_token.shape)\n",
    "print(\"patch features shape:\", patch_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0a58d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# REPO_DIR = r'..\\dinov3'\n",
    "\n",
    "# # DINOv3 ViT models pretrained on web images\n",
    "# model = torch.hub.load(REPO_DIR, 'dinov3_vitl16', source='local', weights=r'..\\dinov3_vitl16_pretrain_lvd1689m-8aa4cbdd.pth')\n",
    "\n",
    "# patch_size = model.config.patch_size\n",
    "# print(\"Patch size:\", patch_size) # 16\n",
    "# print(\"Num register tokens:\", model.config.num_register_tokens) # 4\n",
    "\n",
    "# inputs = processor(images=image, return_tensors=\"pt\")\n",
    "# print(\"Preprocessed image size:\", inputs.pixel_values.shape)  # [1, 3, 224, 224]\n",
    "\n",
    "# batch_size, _, img_height, img_width = inputs.pixel_values.shape\n",
    "# num_patches_height, num_patches_width = img_height // patch_size, img_width // patch_size\n",
    "# num_patches_flat = num_patches_height * num_patches_width\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#   outputs = model(**inputs)\n",
    "\n",
    "# last_hidden_states = outputs.last_hidden_state\n",
    "# print(last_hidden_states.shape)  # [1, 1 + 4 + 256, 384]\n",
    "# assert last_hidden_states.shape == (batch_size, 1 + model.config.num_register_tokens + num_patches_flat, model.config.hidden_size)\n",
    "\n",
    "# cls_token = last_hidden_states[:, 0, :]\n",
    "# patch_features_flat = last_hidden_states[:, 1 + model.config.num_register_tokens:, :]\n",
    "# patch_features = patch_features_flat.unflatten(1, (num_patches_height, num_patches_width))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2726780",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dino' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdino\u001b[49m)\n\u001b[32m      2\u001b[39m out = dino(image)\n",
      "\u001b[31mNameError\u001b[39m: name 'dino' is not defined"
     ]
    }
   ],
   "source": [
    "print(dino)\n",
    "out = dino(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
